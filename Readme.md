# Overview:
This research project seeks to establish an ideal task-offloading strategy for energy-harvesting devices within the context of mobile edge computing (MEC). 
The limited processing power, bandwidth, and battery capacity of mobile devices render them incapable of satisfying the requirements of computationally intensive and time-sensitive communication tasks if not now but in the future. 
This paper formulates an optimization problem with the objective of minimizing the average latency by maximizing throughput under an energy constraint, and proposes a deep learning-based task offloading and time allocation algorithm.

## Results:
![image](https://github.com/bcgrahn/DESIGN-OF-A-MACHINE-LEARNING-BASED-MOBILE-EDGE-COMPUTING-SYSTEM-FOR-IOT-DEVICES/assets/99645139/903d1e12-ba48-42f2-a8ef-a67e346b5be4)

![image](https://github.com/bcgrahn/DESIGN-OF-A-MACHINE-LEARNING-BASED-MOBILE-EDGE-COMPUTING-SYSTEM-FOR-IOT-DEVICES/assets/99645139/c16b55af-3721-4eba-b48c-d4a608ffbbbb)

![image](https://github.com/bcgrahn/DESIGN-OF-A-MACHINE-LEARNING-BASED-MOBILE-EDGE-COMPUTING-SYSTEM-FOR-IOT-DEVICES/assets/99645139/b02b5ed6-ec72-46e3-bbc0-bfb04fc61eec)

![image](https://github.com/bcgrahn/DESIGN-OF-A-MACHINE-LEARNING-BASED-MOBILE-EDGE-COMPUTING-SYSTEM-FOR-IOT-DEVICES/assets/99645139/b4d9e2fb-ca90-4d69-9b31-dc791b931112)

![image](https://github.com/bcgrahn/DESIGN-OF-A-MACHINE-LEARNING-BASED-MOBILE-EDGE-COMPUTING-SYSTEM-FOR-IOT-DEVICES/assets/99645139/23ca2be0-2719-49e8-800e-b1091e486dc1)

This paper investigated an optimal task offloading scheme for energy harvesting devices in mobile edge computing. The purpose is to address the challenges of limited processing power, bandwidth, and battery capacity of MTCD’s making
them unable to meet the demands of computationally intensive and delay-sensitive communication tasks. This paper formulated an optimization problem with the objective of minimizing the average latency by maximizing throughput
under an energy constraint, and proposes a deep learning-based task offloading and time allocation algorithm. The simulation results demonstrate that the computation rate of the proposed implementation can reach 98.87% of the
optimal solution and achieve an 85.9% increase over “random guessing”. The engineering solution effectively fulfilledthe success criteria and outperformed certain existing literature. While guaranteeing system performance, we minimized
tasks dropped, queue build up, and maximised energy harvested. Simulation results show that compared with our greedy benchmark, our proposed supervised DNN model can achieve a comparable task throughput. Finally, the numerical
results demonstrate the rapid flexibility of our model as it takes an average of 1.04 frames to converge back to theoptimal solution.
